---
title: 字符集
date: 2025-12-10 20:58:04
tags: CS
---

## 1. 基本概念梳理

在编码相关问题中，最容易混淆的几个概念是：

- **字符（Character）**
   人类眼中的一个符号，例如：`A`、`中`、`🙂`。
- **码点（Code Point）**
   给字符分配的唯一编号。
   例如在 Unicode 中：
  - `A` → U+0041
  - `中` → U+4E2D
- **字符集（Character Set / Charset）**
   一套“有哪些字符 + 每个字符对应什么码点”的**字符集合与编号表**。
   可以理解为一本“字典”，定义了可用字符范围和每个字符的编号。
- **字符编码（Character Encoding）**
   规定“码点 → 字节序列”的转换规则，即**字符最终在文件/网络中用哪些字节表示**。
   例如同一个码点 U+4E2D：
  - 在 UTF-8 中编码为 `E4 B8 AD`
  - 在 UTF-16LE 中编码为 `2D 4E`
- **字体（Font）**
   决定这个字符“长什么样”，与字符集/编码不同层面。

在很多系统/配置中，`charset` 这个词往往同时指“字符集 + 编码实现方式”（例如 `charset=utf-8`）。

------

## 2. 为什么会有那么多字符集？

### 2.1 ASCII：起点

最早只有美国用户使用计算机，设计了 **ASCII**：

- 7 位编码（0–127），共 128 个字符
- 包含英文字母、数字、常用标点和控制字符
- 每个字符 1 字节，高位为 0

当时只考虑英文，世界看起来很简单。

### 2.2 各国各搞一套本地编码

随着计算机全球使用：

- 欧洲语言需要带重音的拉丁字母（é、ö、ç 等）
- 中文需要大量汉字
- 日文需要平假名/片假名/汉字
- 韩文有自己的字母系统

受制于早期硬件资源，大多方案希望保持**单字节编码（1 字节 256 种可能）**：

- 0–127 继续兼容 ASCII
- 128–255 这部分，各地区/厂商自由发挥

结果出现了大量地区性/厂商自定义字符集：

- 欧洲：`ISO-8859-1/2/...` 等
- 简体中文：`GB2312` → `GBK` → `GB18030`
- 繁体中文：`Big5`
- 日文：`Shift_JIS`、`EUC-JP`
- 各种 Windows 代码页：`cp936`、`cp1252` 等

这些字符集对同一字节（例如 `0xE4`）的解释完全不同，互不兼容，因此产生大量乱码问题。

### 2.3 Unicode：统一大一统方案

为解决“各国各搞一套彼此不兼容”的混乱局面，提出了 **Unicode**：

- 一套统一的“全球字符 → 码点”映射规范
- 覆盖几乎所有自然语言文字与大量符号

注意：
 **Unicode 本身是“码点规范”，不是具体存储格式。**
 在 Unicode 之上，才有不同的编码方式：

- **UTF-8**：变长 1–4 字节，兼容 ASCII，是互联网标准默认编码
- **UTF-16**：多数字符 2 字节，少数 4 字节，常见于 Windows/Java 内部
- **UTF-32**：每个字符固定 4 字节，简单但浪费空间

------

## 3. 常见字符集与编码方式

### 3.1 旧式地区性编码（单/双字节）

- **ASCII**：只含英文，1 字节
- **ISO-8859-x 系列**：面向不同欧洲语言的扩展
- **GB2312 / GBK / GB18030**：简体中文编码演进
- **Big5**：繁体中文
- **Shift_JIS / EUC-JP**：日文编码

特点：
 覆盖范围有限，只适合特定地区/语言，互不兼容。

### 3.2 Unicode 编码系列

- **UTF-8**
  - 变长，1–4 字节
  - ASCII 范围仍然是单字节，与 ASCII 完全兼容
  - 非 ASCII 字符使用 2–4 字节
  - 目前实际工程中首选编码
- **UTF-16**
  - 大多字符 2 字节，少数增补字符 4 字节
  - 有大小端之分：UTF-16LE / UTF-16BE
  - Windows/Java 内存中常见
- **UTF-32**
  - 每个字符固定 4 字节
  - 实现简单，但空间开销大

------

## 4. UTF-8 与 UTF-8 BOM 的区别

### 4.1 BOM 的作用

**BOM（Byte Order Mark，字节序标记）**：

- 最初为 UTF-16 / UTF-32 这类存在**大小端问题**的编码设计：
  - UTF-16 BE：`FE FF`
  - UTF-16 LE：`FF FE`
- 对应 Unicode 字符 U+FEFF（零宽不换行空格），放在文件开头，帮助解码器判断字节序。

**UTF-8 本身不存在大小端问题**，理论上不需要 BOM。
 但某些历史原因（主要是 Windows 记事本识别编码），导致出现了 “UTF-8 带 BOM” 的实际写法。

### 4.2 UTF-8 vs UTF-8 BOM

- **普通 UTF-8（无 BOM）**：
   文件内容完全由有效字符的 UTF-8 字节组成。
- **UTF-8 with BOM**：
   文件开头额外多出 3 个字节：`EF BB BF`（UTF-8 编码的 U+FEFF）。
   之后字符编码与普通 UTF-8 完全一致。

**本质：**

> UTF-8 和 UTF-8 BOM 的编码规则完全相同，只是“带 BOM”形式文件开头多 3 个字节。

### 4.3 BOM 带来的兼容性问题

一些工具将 BOM 当作“提示”，另一些则把它当作普通字节，可能产生问题：

- 脚本语言/shebang：
  - `#!/usr/bin/env python` 前面多了 BOM，会导致某些环境无法正确识别脚本解释器。
- JSON / 配置文件：
  - 部分解析库不接受 BOM，可能报错或产生不可见字符。
- PHP / HTTP 头：
  - 若 PHP 文件为 UTF-8 BOM，BOM 会被输出到响应流头部，导致 `header()` 报 “Headers already sent”。
- SQL 脚本 / 文件名等：
  - 开头的 BOM 被当作语句内容或名称一部分，引发语法问题或“看不见但存在”的脏数据。

### 4.4 工程实践建议

- **后端代码、脚本、配置文件、SQL、JSON 等**
   → 推荐统一使用 **UTF-8（无 BOM）**，兼容性最好。
- **仅为兼容非常旧的 Windows Notepad 或 Excel 打开 CSV**
   → 可根据需要导出带 BOM 的 UTF-8 文件，改善其自动识别能力。

------

## 5. MySQL 中的 `utf8` 与 `utf8mb4`

### 5.1 `utf8` 与标准 UTF-8 的不一致

**标准 UTF-8** 允许使用 1–4 字节表示 Unicode 字符，其中：

- U+0000 ~ U+007F：1 字节
- U+0080 ~ U+07FF：2 字节
- U+0800 ~ U+FFFF：3 字节
- U+10000 ~ U+10FFFF：4 字节（增补平面，包括 emoji 等）

**MySQL 的 `utf8` 字符集** 实际上只支持最多 3 字节：

- 即仅支持 U+0000 ~ U+FFFF（基本多文种平面）
- 对 4 字节字符（如 emoji、部分特殊汉字）无法存储

因此：

> MySQL 的 `utf8` 是“阉割版 UTF-8”，并非完整 UTF-8。

### 5.2 `utf8mb4`：MySQL 对“完整 UTF-8”的实现

**`utf8mb4`（multi-byte up to 4 bytes）**：

- 支持 1–4 字节 UTF-8 编码
- 可以覆盖全部 Unicode 码点，包括 emoji 和扩展字符
- 在字节层面就是标准 UTF-8

因此：

- `utf8mb4` = 真·完整 UTF-8
- `utf8`     = MySQL 专用的 3 字节“缩水版 UTF-8”

### 5.3 使用场景与问题

- 使用 `utf8` 作为列字符集时：
  - 无法插入 emoji、部分特殊符号，可能导致：
    - 插入失败
    - 自动截断
    - 替换为 `?` 等占位符
- 使用 `utf8mb4`：
  - 可以安全存储所有 Unicode 字符
  - 与前端/后端统一 UTF-8 编码完全兼容

### 5.4 推荐配置示例

**数据库级**：

```sql
CREATE DATABASE mydb
  DEFAULT CHARACTER SET utf8mb4
  DEFAULT COLLATE utf8mb4_general_ci;
```

**表与字段级**：

```sql
CREATE TABLE user_info (
  id      BIGINT PRIMARY KEY,
  name    VARCHAR(64) CHARACTER SET utf8mb4,
  comment TEXT        CHARACTER SET utf8mb4
);
```

**JDBC 连接示例（Java）**：

```text
jdbc:mysql://host:3306/mydb
  ?useUnicode=true
  &characterEncoding=utf8mb4
```

建议：新项目统一使用 `utf8mb4`，并保证：

> 前端（UTF-8） → 后端语言（UTF-8） → 数据库（utf8mb4）
>  全链路统一编码，避免隐含编码转换。

------

## 6. 字符集差异在编程中的典型问题

### 6.1 乱码与存储错误

典型错误示例（MySQL）：

```text
Incorrect string value '\xE6\xB2\x89\xE9\xBB\x98...' for column 'AUTHOR'
```

含义：

- 实际发送的字节是 UTF-8 编码的中文（如 `沉默...`）
- 数据库列使用的字符集（如 `latin1` 或错误配置的 `utf8`）不接受该字节序列
- MySQL 尝试按列字符集解码时发现非法字节 → 报错

**本质原因：**
 发送端与接收端对“这些字节是什么编码”意见不一致。

### 6.2 字符长度与截断问题

多字节编码（UTF-8、GBK 等）下，**字符数 ≠ 字节数**：

- 数据库字段 `VARCHAR(10)` 通常是 10 个“字符”
- 但底层可能最多占用 30~40 字节空间（UTF-8）
- 如果应用层代码按字节数截断，可能截断到一个字符中间，生成非法编码序列

常见问题：

- UI 截断导致半个汉字显示异常
- 后端截断后插入数据库时出现编码错误或报错

### 6.3 边界处编码不一致

常见边界包括：

- 源文件编码 vs 编译器/解释器认为的编码
- HTTP 请求/响应体实际编码 vs `Content-Type` 中声明的 charset
- 终端/控制台编码（Windows 旧版 CMD 默认 GBK，Linux 常为 UTF-8）
- 文件系统对文件名使用的编码

任一环节不一致都会导致乱码，严重时引发程序异常。

### 6.4 排序/比较/Collation 相关问题

字符集与 Collation 共同影响：

- 字符比较是否大小写敏感
- 特殊字符/带音标字符的比较与排序顺序
- 唯一索引是否视某些变体为相同（例如 `ß` 与 `ss` 等）

在数据库中，`charset + collation` 的组合会直接影响：

- `ORDER BY` 排序结果
- `WHERE col = '值'` 的匹配行为
- 唯一约束冲突判断

------

## 7. 实战中的通用建议

1. **统一标准：优先使用 UTF-8（无 BOM）+ MySQL utf8mb4**
   - 编辑器、源代码、配置文件、脚本、HTTP 接口：全部使用 UTF-8（无 BOM）
   - 数据库使用 `utf8mb4`，包括：
     - `character_set_server` / `character_set_database`
     - 表与字段定义
2. **明确每个“边界”的编码**
   - 网络协议（HTTP、WebSocket 等）明示 charset
   - 数据库驱动连接参数中显式设置编码
   - 打开文件时通过 API 指定编码，不依赖系统默认值
3. **遇到编码问题时的排查顺序**
   - 数据原始来源（前端/文件/接口）的真实编码
   - 应用程序对输入/输出流的编码设定
   - 数据库驱动与连接的编码配置
   - 数据库表/列的字符集与 Collation

只要做到：**全链路编码一致 + 清晰标识 + 不依赖隐式默认值**，大部分字符集相关问题（乱码、报错、截断）都可以避免。

------

如果你后续需要，我可以在这个总结的基础上，帮你再写一份专门面向你当前项目的「编码规范与检查清单」，比如：

- Spring Boot / MyBatis / MySQL 的统一 UTF-8mb4 配置模板
- 常见编码问题的排查脚本与 SQL 命令示例
